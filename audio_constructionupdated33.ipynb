{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUDHARSAN270/Machine_learning/blob/main/audio_constructionupdated33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "Hod4P2vwo3fA"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\n",
        "    root='/media',\n",
        "    url='train-clean-100',\n",
        "    download=True\n",
        ")"
      ],
      "metadata": {
        "id": "tFkoTfOCLJUH",
        "outputId": "6cdd3b3d-104d-43e7-df80-484c9f60c526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [02:42<00:00, 39.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_dataset[0]\n",
        "\n",
        "print(\"Waveform: \", waveform.shape)\n",
        "print(\"Sample Rate: \", sample_rate)\n",
        "print(\"Utterance: \", utterance)\n",
        "print(\"Speaker ID: \", speaker_id)\n",
        "print(\"Chapter ID: \", chapter_id)\n",
        "print(\"Utterance ID: \", utterance_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhBwIah-99kL",
        "outputId": "493136f1-e409-4f00-ba40-5c06ec142f2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveform:  torch.Size([1, 225360])\n",
            "Sample Rate:  16000\n",
            "Utterance:  CHAPTER ONE MISSUS RACHEL LYNDE IS SURPRISED MISSUS RACHEL LYNDE LIVED JUST WHERE THE AVONLEA MAIN ROAD DIPPED DOWN INTO A LITTLE HOLLOW FRINGED WITH ALDERS AND LADIES EARDROPS AND TRAVERSED BY A BROOK\n",
            "Speaker ID:  103\n",
            "Chapter ID:  1240\n",
            "Utterance ID:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SoundProcessingModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SoundProcessingModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Output: 32 x 32 x 3\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # Output: 64 x 16 x 1\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        # Adjust the kernel size or stride of pool3 to avoid reducing the dimension to 0\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0)  # Output: 128 x 8 x 1\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(input_size=128 * 8 , hidden_size=128, batch_first=True)  # Adjusted based on new input size\n",
        "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)\n",
        "\n",
        "        self.fc1 = nn.Linear(64, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))  # 1x64x7 -> 32x32x3\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))  # 32x32x3 -> 64x16x1\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))  # 64x16x1 -> 128x8x1\n",
        "\n",
        "        x = self.flatten(x)  # Shape: (batch_size, 128 * 8 * 1)\n",
        "        x = x.unsqueeze(1)  # Add sequence dimension: Shape: (batch_size, 1, 128 * 8 * 1)\n",
        "\n",
        "        x, _ = self.lstm1(x)  # Output shape: (batch_size, seq_len, 128)\n",
        "        x, _ = self.lstm2(x)  # Output shape: (batch_size, seq_len, 64)\n",
        "        x = x[:, -1, :]  # Get the output of the last time step: Shape: (batch_size, 64)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))  # Shape: (batch_size, 128)\n",
        "        x = self.fc2(x)  # Shape: (batch_size, num_classes)\n",
        "        return x\n",
        "\n",
        "# Define the model\n",
        "model = SoundProcessingModel(num_classes=10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4v92RG2ciWNh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform  = transforms.MelSpectrogram(\n",
        "    sample_rate = 16000,\n",
        "    n_mels =64\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "class CustomAudioDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, dataset, transform = transform, fixed_length = 1227):\n",
        "    self.dataset = dataset\n",
        "    self.transform = transform\n",
        "    self.fixed_length = fixed_length\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    waveform,sample_rate,utterance,speaker_id, character_id, utterance_id = self.dataset[idx]\n",
        "     # Apply padding or truncation if fixed_length is specified\n",
        "    if self.fixed_length is not None:\n",
        "      waveform = F.pad(waveform, (0, self.fixed_length - waveform.size(-1)))\n",
        "      waveform = waveform[:,:self.fixed_length]\n",
        "\n",
        "    if self.transform:\n",
        "      waveform = self.transform(waveform)\n",
        "      # Return only the waveform and character_id as a Tensor\n",
        "      return waveform, torch.tensor(character_id) # Convert character_id to a Tensor\n",
        "\n",
        "\n",
        "customdataset = CustomAudioDataset(train_dataset)\n",
        "data_loader = DataLoader(customdataset, batch_size=10, shuffle=False)\n"
      ],
      "metadata": {
        "id": "AqIogKE-2QRB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the data loader and feed batches to the model\n",
        "for batch_idx, (waveform, utterance) in enumerate(data_loader):\n",
        "    print(f\"Batch {batch_idx} - Waveform: {waveform.shape}\")\n",
        "    print(f\"Batch {batch_idx} - Utterance: {utterance}\")\n",
        "\n",
        "    # Move data to the appropriate device (CPU/GPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    waveform = waveform.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(waveform)\n",
        "\n",
        "    print(f\"Batch {batch_idx} - Model Output: {output.shape}\")\n",
        "    print(f\"Batch {batch_idx} - Model Output: {output}\")\n",
        "\n",
        "    if batch_idx == 1:  # Just to prevent printing too many items, remove this condition to print all\n",
        "        break\n"
      ],
      "metadata": {
        "id": "ObJadR1DiT0f",
        "outputId": "92cf4bc9-9f17-4899-aa60-5bbf19ba4bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 - Waveform: torch.Size([10, 1, 64, 7])\n",
            "Batch 0 - Utterance: tensor([1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240])\n",
            "Batch 0 - Model Output: torch.Size([10, 1227])\n",
            "Batch 0 - Model Output: tensor([[-0.0441, -0.0305, -0.0026,  ..., -0.0627, -0.0341,  0.0387],\n",
            "        [-0.0634,  0.0548, -0.0386,  ...,  0.0149, -0.0283,  0.0428],\n",
            "        [-0.0667,  0.0070, -0.0029,  ...,  0.0154, -0.0044,  0.0421],\n",
            "        ...,\n",
            "        [-0.0791,  0.0521, -0.0424,  ..., -0.0147, -0.0220, -0.0240],\n",
            "        [-0.0452,  0.0264, -0.0321,  ..., -0.0670, -0.0338, -0.0277],\n",
            "        [-0.0640,  0.0310, -0.0156,  ..., -0.0240, -0.0714, -0.0307]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Batch 1 - Waveform: torch.Size([10, 1, 64, 7])\n",
            "Batch 1 - Utterance: tensor([1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240, 1240])\n",
            "Batch 1 - Model Output: torch.Size([10, 1227])\n",
            "Batch 1 - Model Output: tensor([[-0.0132,  0.0690,  0.0145,  ..., -0.0341, -0.0401,  0.0406],\n",
            "        [-0.0252, -0.0444, -0.0233,  ..., -0.0722, -0.0163,  0.0464],\n",
            "        [-0.0228, -0.0097, -0.0674,  ..., -0.0575, -0.0646,  0.0801],\n",
            "        ...,\n",
            "        [-0.0412,  0.0011, -0.0785,  ..., -0.0253,  0.0411,  0.0118],\n",
            "        [-0.0398, -0.0126, -0.0294,  ..., -0.0223,  0.0120,  0.0325],\n",
            "        [-0.1077,  0.0489, -0.0388,  ..., -0.0604, -0.0582,  0.0223]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(customdataset, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "Aj0vQ9LyuIux"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming character_ids is a list or NumPy array containing all target labels\n",
        "unique_classes, counts = np.unique(character_id ,return_counts=True)\n",
        "num_classes = len(unique_classes)\n",
        "print(\"Number of classes:\", num_classes)"
      ],
      "metadata": {
        "id": "Mv8Q1EErtvWs",
        "outputId": "72ea999f-8402-4a57-8ec3-bbb8612ef8e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}